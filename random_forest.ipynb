{"cells":[{"cell_type":"markdown","source":["# IST 718: Big Data Analytics\n\n- Professor: Willard Williamson <wewillia@syr.edu>\n- Faculty Assistant: Palaniappan Muthukkaruppan\n## General instructions:\n\n- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers from your classmates.  Short code snippets are allowed from the internet.  Any code is allowed from the class text books or class provided code.__\n- Please do not change the file names. The FAs and the professor use these names to grade your homework.\n- Remove or comment out code that contains `raise NotImplementedError`. This is mainly to make the `assert` statement fail if nothing is submitted.\n- The tests shown in some cells (i.e., `assert` and `np.testing.` statements) are used to grade your answers. **However, the professor and FAs will use __additional__ test for your answer. Think about cases where your code should run even if it passess all the tests you see.**\n- Before submitting your work through Blackboard, remember to save and press `Validate` (or go to \n`Kernel`$\\rightarrow$`Restart and Run All`)."],"metadata":{"deletable":false,"editable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-b038e38b5e3072a9","locked":true,"solution":false,"checksum":"c51f80b694da894627ba37be28c86055","grade":false}}},{"cell_type":"code","source":["# load these packages\nimport pyspark\nfrom pyspark.ml import feature, classification\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import functions as fn\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml import feature, regression, evaluation, Pipeline\nfrom pyspark.sql import functions as fn, Row\nimport matplotlib.pyplot as plt\nspark = SparkSession.builder.getOrCreate()\nsc = spark.sparkContext\nimport pandas as pd\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nimport os\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["The following cell is used to determine if the environment is databricks or personal computer and load the csv file accordingly."],"metadata":{}},{"cell_type":"code","source":["def get_training_dataframe(data_file_name):  \n    # get the databricks runtime version\n    db_env = os.getenv(\"DATABRICKS_RUNTIME_VERSION\")\n    grading_env = os.getenv(\"GRADING_RUNTIME_ENV\")\n    \n    # if the databricks env var exists\n    if db_env != None:\n        full_path_name = \"/FileStore/tables/%s\" % data_file_name\n    elif grading_env != None:\n        full_path_name = \"C:/Users/Will/Desktop/SU/datasets/%s\" % data_file_name\n    else:\n        full_path_name = data_file_name\n        \n    return spark.read.csv(full_path_name, inferSchema=True, header=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["# Random Forest"],"metadata":{}},{"cell_type":"markdown","source":["This assignment gives you some practice using the spark documentation to figure out how to do some common tasks on your own.  Note that there is a subjective component to the grading of this assignment.  Graders reserve the right to take points off for lack of effort or poorly presented explanations."],"metadata":{}},{"cell_type":"markdown","source":["In these questions, we will examine the famous Titanic dataset"],"metadata":{}},{"cell_type":"markdown","source":["[Column Descriptions](https://data.world/nrippner/titanic-disaster-dataset): <br>\nsurvival - Survival (0 = No; 1 = Yes) <br>\nclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) <br>\nname - Name <br>\nsex - Sex <br>\nage - Age <br>\nsibsp - Number of Siblings/Spouses Aboard <br>\nparch - Number of Parents/Children Aboard <br>\nticket - Ticket Number <br>\nfare - Passenger Fare <br>\ncabin - Cabin <br>\nembarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) <br>\nboat - Lifeboat (if survived) <br>\nbody - Body number (if did not survive and body was recovered) <br>"],"metadata":{}},{"cell_type":"code","source":["# read-only\ntitanic_df = get_training_dataframe(\"titanic_original.csv\")\n\ndrop_cols = ['boat', 'body']\ntitanic_df = titanic_df.\\\n    drop(*drop_cols).\\\n    fillna('O').\\\n    dropna(subset=['pclass', 'age', 'sibsp', 'parch', 'fare', 'survived']).\\\n    select((fn.col('sex') == 'male').alias('is_male').cast('float'),           \n           'pclass',\n           'age',\n           'sibsp',\n           'parch',\n           'fare',\n           'survived')\ntraining_df, testing_df = titanic_df.randomSplit([0.7, 0.3], seed=0)\ntitanic_df.printSchema()"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-219523fd6f45f014","locked":true,"solution":false,"checksum":"f0c5e45b6f138ba0d550dff214ce7ee7","grade":false}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- is_male: float (nullable = false)\n-- pclass: integer (nullable = true)\n-- age: double (nullable = true)\n-- sibsp: integer (nullable = true)\n-- parch: integer (nullable = true)\n-- fare: double (nullable = true)\n-- survived: integer (nullable = true)\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["titanic_df.show(10)"],"metadata":{"scrolled":true},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------+------+-----+-----+--------+--------+\nis_male|pclass|   age|sibsp|parch|    fare|survived|\n+-------+------+------+-----+-----+--------+--------+\n    0.0|     1|  29.0|    0|    0|211.3375|       1|\n    1.0|     1|0.9167|    1|    2|  151.55|       1|\n    0.0|     1|   2.0|    1|    2|  151.55|       0|\n    1.0|     1|  30.0|    1|    2|  151.55|       0|\n    0.0|     1|  25.0|    1|    2|  151.55|       0|\n    1.0|     1|  48.0|    0|    0|   26.55|       1|\n    0.0|     1|  63.0|    1|    0| 77.9583|       1|\n    1.0|     1|  39.0|    0|    0|     0.0|       0|\n    0.0|     1|  53.0|    2|    0| 51.4792|       1|\n    1.0|     1|  71.0|    0|    0| 49.5042|       0|\n+-------+------+------+-----+-----+--------+--------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["# Question 1: (10 pts)\nCreate a spark RandomForestClassifier using all default parameters.  Train the model and calculate the AUC using a BinaryClassificationEvaluator."],"metadata":{}},{"cell_type":"code","source":["# Your code here\nrf_assembler = VectorAssembler(inputCols=training_df.columns[0:6], outputCol=\"features\")\nrf = classification.RandomForestClassifier(featuresCol = \"features\", labelCol = \"survived\")\nrf_pipeline = Pipeline(stages=[rf_assembler, rf]).fit(training_df)\nbce = BinaryClassificationEvaluator(labelCol='survived')\ndefault_rf_model = bce.evaluate(rf_pipeline.transform(testing_df))\nprint(default_rf_model)\n#raise NotImplementedError()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.8540761132039021\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["# Question 2: (10 pts)\nUse spark RandomForestClassifier, ParamGridBuilder, and CrossValidator objects to perform a random forest grid search.  Use 3 fold cross validation and a BinaryClassificationEvaluator to evaluate the results.  The goal is to see if you can improve upon the AUC score produced by the default random forest model above.  Obvious things that you might consider varying in the grid search include the number of randomly selected columns for each split point, the number of trees in the forest, and the impurity measurement (gini / entropy).  You are free to choose any hyper parameters you want in your grid search."],"metadata":{}},{"cell_type":"code","source":["# Your grid search code here\nparamGrid = (ParamGridBuilder()\n             .addGrid(rf.numTrees, [20,24])\n             .addGrid(rf.maxDepth, [6,10])\n             .addGrid(rf.impurity,['gini', 'entropy'])\n             .build())\n\ncv_rf= CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=bce, numFolds=3)\n# Run cross validations\ncv_rf_Model = Pipeline(stages = [rf_assembler, cv_rf]).fit(training_df)\n#raise NotImplementedError()"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-5d34f31b40350198","locked":false,"solution":true,"checksum":"2912f0aa0562283b8c2b9b8d330a9e8c","grade":false}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["print(bce.evaluate(cv_rf_Model.transform(testing_df)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.8601130107215299\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["rf_prediction = cv_rf_Model.transform(testing_df)\nrf_prediction.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------+----+-----+-----+--------+--------+--------------------+--------------------+--------------------+----------+\nis_male|pclass| age|sibsp|parch|    fare|survived|            features|       rawPrediction|         probability|prediction|\n+-------+------+----+-----+-----+--------+--------+--------------------+--------------------+--------------------+----------+\n    0.0|     1| 2.0|    1|    2|  151.55|       0|[0.0,1.0,2.0,1.0,...|[1.86019478797221...|[0.09300973939861...|       1.0|\n    0.0|     1|16.0|    0|    0|    86.5|       1|[0.0,1.0,16.0,0.0...|[0.44330746817227...|[0.02216537340861...|       1.0|\n    0.0|     1|16.0|    0|    1|    39.4|       1|[0.0,1.0,16.0,0.0...|[0.74969112116717...|[0.03748455605835...|       1.0|\n    0.0|     1|16.0|    0|    1| 57.9792|       1|[0.0,1.0,16.0,0.0...|[0.39534630312373...|[0.01976731515618...|       1.0|\n    0.0|     1|17.0|    1|    0|   108.9|       1|[0.0,1.0,17.0,1.0...|[0.44330746817227...|[0.02216537340861...|       1.0|\n    0.0|     1|21.0|    2|    2| 262.375|       1|[0.0,1.0,21.0,2.0...|[0.89534630312373...|[0.04476731515618...|       1.0|\n    0.0|     1|22.0|    0|    1| 61.9792|       1|[0.0,1.0,22.0,0.0...|[0.39534630312373...|[0.01976731515618...|       1.0|\n    0.0|     1|22.0|    0|    2|    49.5|       1|[0.0,1.0,22.0,0.0...|[0.62342849490455...|[0.03117142474522...|       1.0|\n    0.0|     1|23.0|    1|    0| 82.2667|       1|[0.0,1.0,23.0,1.0...|[0.44330746817227...|[0.02216537340861...|       1.0|\n    0.0|     1|23.0|    1|    0| 113.275|       1|[0.0,1.0,23.0,1.0...|[0.44330746817227...|[0.02216537340861...|       1.0|\n    0.0|     1|24.0|    0|    0|    69.3|       1|[0.0,1.0,24.0,0.0...|[0.44330746817227...|[0.02216537340861...|       1.0|\n    0.0|     1|27.0|    1|    1|247.5208|       1|[0.0,1.0,27.0,1.0...|[0.50060946101846...|[0.02503047305092...|       1.0|\n    0.0|     1|28.0|    3|    2|   263.0|       1|[0.0,1.0,28.0,3.0...|[1.00060946101846...|[0.05003047305092...|       1.0|\n    0.0|     1|29.0|    0|    0|211.3375|       1|[0.0,1.0,29.0,0.0...|[0.54857062606701...|[0.02742853130335...|       1.0|\n    0.0|     1|30.0|    0|    0|    93.5|       1|[0.0,1.0,30.0,0.0...|[0.54857062606701...|[0.02742853130335...|       1.0|\n    0.0|     1|30.0|    0|    0| 106.425|       1|[0.0,1.0,30.0,0.0...|[0.54857062606701...|[0.02742853130335...|       1.0|\n    0.0|     1|32.0|    0|    0| 76.2917|       1|[0.0,1.0,32.0,0.0...|[0.54857062606701...|[0.02742853130335...|       1.0|\n    0.0|     1|33.0|    0|    0|    86.5|       1|[0.0,1.0,33.0,0.0...|[0.54857062606701...|[0.02742853130335...|       1.0|\n    0.0|     1|35.0|    0|    0|135.6333|       1|[0.0,1.0,35.0,0.0...|[0.54857062606701...|[0.02742853130335...|       1.0|\n    0.0|     1|35.0|    1|    0|    52.0|       1|[0.0,1.0,35.0,1.0...|[0.71107062606701...|[0.03555353130335...|       1.0|\n+-------+------+----+-----+-----+--------+--------+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["# Question 3 (10 pts)\nPrint the AUC and hyper parameters of the best random forest model in the code cell below.  Describe in words in the markdown cell below what specific parameters you used in your grid and what those parameters do in the model.  For example, if you specified impurity in your grid, describe what impurity does in the random forest.  Keep the descriptions brief and at a high level, I'm just trying to see if you understand the high level concept of what the tuning parameter does."],"metadata":{}},{"cell_type":"code","source":["# Your AUC code here\nbest_model_rf = cv_rf_Model.stages[-1].bestModel\nbest_model_rf.extractParamMap()\n#raise NotImplementedError()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[115]: {Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;cacheNodeIds&#39;, doc=&#39;If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.&#39;): False,\n Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;checkpointInterval&#39;, doc=&#39;set checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext&#39;): 10,\n Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;featureSubsetStrategy&#39;, doc=&#39;The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].&#39;): &#39;auto&#39;,\n Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;featuresCol&#39;, doc=&#39;features column name&#39;): &#39;features&#39;,\n Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;impurity&#39;, doc=&#39;Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini&#39;): &#39;entropy&#39;,\n Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;labelCol&#39;, doc=&#39;label column name&#39;): &#39;survived&#39;,\n Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;maxBins&#39;, doc=&#39;Max number of bins for discretizing continuous features.  Must be at least 2 and at least number of categories for any categorical feature.&#39;): 32,\n Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;maxDepth&#39;, doc=&#39;Maximum depth of the tree. (Nonnegative) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.&#39;): 6,\n Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;maxMemoryInMB&#39;, doc=&#39;Maximum memory in MB allocated to histogram aggregation.&#39;): 256,\n Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;minInfoGain&#39;, doc=&#39;Minimum information gain for a split to be considered at a tree node.&#39;): 0.0,\n Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;minInstancesPerNode&#39;, doc=&#39;Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Must be at least 1.&#39;): 1,\n Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;numTrees&#39;, doc=&#39;Number of trees to train (at least 1)&#39;): 20,\n Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;predictionCol&#39;, doc=&#39;prediction column name&#39;): &#39;prediction&#39;,\n Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;probabilityCol&#39;, doc=&#39;Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities&#39;): &#39;probability&#39;,\n Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;rawPredictionCol&#39;, doc=&#39;raw prediction (a.k.a. confidence) column name&#39;): &#39;rawPrediction&#39;,\n Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;seed&#39;, doc=&#39;random seed&#39;): -5387697053847413545,\n Param(parent=&#39;RandomForestClassifier_e1e626d1b679&#39;, name=&#39;subsamplingRate&#39;, doc=&#39;Fraction of the training data used for learning each decision tree, in range (0, 1].&#39;): 1.0}</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["Write your grid search parameter descriptions here:"],"metadata":{}},{"cell_type":"markdown","source":["```\nnumTrees (20):Increasing the number of trees will decrease the variance in predictions, improving the modelâ€™s test-time accuracy. \nmaxDepth(6): Increasing the depth makes the model more expressive and powerful. However, deep trees take longer to train and are also more prone to overfitting.\nimpurity(entropy): The node impurity is a measure of the homogeneity of the labels at the node. This measure must match the algo parameter.\n```"],"metadata":{}},{"cell_type":"markdown","source":["# Question 4: 10 pts\nCreate a pandas dataframe `feature_importance` with the columns `feature` and `importance` which contains the names of the features (`is_male`, `pclass`, etc.) and their feature importance as determined by the random forest model. Sort the dataframe by `importance` in descending order. In the markdown cell, add comments on the importance that random forest has given to each feature. Are they reasonable? Do they tell you anything valuable about the titanic dataset?"],"metadata":{}},{"cell_type":"code","source":["# Your code here\nimportances = rf_pipeline.stages[-1].featureImportances.toArray()\nfeatures = training_df.columns[0:6]\n\nfeature_importance = pd.DataFrame(list(zip(features,importances)),\n                                 columns = ['feature', 'importance']).sort_values('importance',ascending = False)\nfeature_importance\n#raise NotImplementedError()"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-5b666b921af5a375","locked":false,"solution":true,"checksum":"89dc0c52dff983147af006f48049dd3e","grade":false}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>is_male</td>\n      <td>0.500845</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pclass</td>\n      <td>0.154806</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>fare</td>\n      <td>0.133774</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>age</td>\n      <td>0.128699</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sibsp</td>\n      <td>0.055356</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>parch</td>\n      <td>0.026521</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["### Your feature importance comments here.\nAccording to the default model of Random Forest, the most important feature is 'ismale' so maybe that means that since men are physically stronger than women, they would have a better chance of survivng.\n\nThe second highest feature is 'fare' which makes sense to me as customers who've paid more for a better class may have the first priority to save their life by getting out of the ship first.\n\nThe third highest feature also makes sense because higher the passenger class, better the services provided would be, which includes safety and life boats. \n\nAge also could be a factor, since it is easier to save your life when you are young compared to older people. Also, the younger you are, the more prone your body is to take a shock well than older people.\n\nThe last two features dont make a lot of sense to me because logically we would give first prefernce to kids, followed by elder people & females and then men."],"metadata":{}},{"cell_type":"markdown","source":["# Question 5:  10 pts.\nPrint any of the trees in the forest from the final model.  Copy the printed text to the tree printout markdown cell below.  Add comments to the markdown cell below describing how the root node is split:  What variable is being split and what is the value that determines the left / right split.  It is important to copy the printed tree output to the tree pritout markdown cell because the trees are grown with random parameters and the graders will get a totally different tree when we run your code."],"metadata":{}},{"cell_type":"code","source":["# your code here\nprint(rf_pipeline.stages[-1].trees[3].toDebugString)\n#raise NotImplementedError()"],"metadata":{"deletable":false,"nbgrader":{"schema_version":1,"grade_id":"cell-7476baa7ceb72c05","locked":false,"solution":true,"checksum":"b212587e3d2f28ad35f04e2a6149e762","grade":false}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">DecisionTreeClassificationModel (uid=dtc_bff43c7859c1) of depth 5 with 29 nodes\n  If (feature 0 &lt;= 0.5)\n   If (feature 2 &lt;= 49.5)\n    If (feature 1 &lt;= 2.5)\n     Predict: 1.0\n    Else (feature 1 &gt; 2.5)\n     If (feature 2 &lt;= 26.25)\n      Predict: 1.0\n     Else (feature 2 &gt; 26.25)\n      Predict: 0.0\n   Else (feature 2 &gt; 49.5)\n    If (feature 1 &lt;= 1.5)\n     Predict: 1.0\n    Else (feature 1 &gt; 1.5)\n     If (feature 5 &lt;= 24.075)\n      Predict: 1.0\n     Else (feature 5 &gt; 24.075)\n      If (feature 2 &lt;= 54.5)\n       Predict: 1.0\n      Else (feature 2 &gt; 54.5)\n       Predict: 0.0\n  Else (feature 0 &gt; 0.5)\n   If (feature 5 &lt;= 27.825)\n    If (feature 5 &lt;= 18.375)\n     Predict: 0.0\n    Else (feature 5 &gt; 18.375)\n     If (feature 2 &lt;= 2.5)\n      Predict: 1.0\n     Else (feature 2 &gt; 2.5)\n      Predict: 0.0\n   Else (feature 5 &gt; 27.825)\n    If (feature 3 &lt;= 2.5)\n     If (feature 2 &lt;= 17.5)\n      If (feature 1 &lt;= 2.5)\n       Predict: 1.0\n      Else (feature 1 &gt; 2.5)\n       Predict: 0.0\n     Else (feature 2 &gt; 17.5)\n      If (feature 5 &lt;= 30.5979)\n       Predict: 1.0\n      Else (feature 5 &gt; 30.5979)\n       Predict: 0.0\n    Else (feature 3 &gt; 2.5)\n     Predict: 0.0\n\n</div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["## Paste the tree printout in this cell:\n\nDecisionTreeClassificationModel (uid=dtc_ab3151614376) of depth 5 with 51 nodes\n\n```\nIf (feature 0 <= 0.5)\n   If (feature 2 <= 49.5)\n    If (feature 1 <= 2.5)\n     Predict: 1.0\n    Else (feature 1 > 2.5)\n     If (feature 2 <= 26.25)\n      Predict: 1.0\n     Else (feature 2 > 26.25)\n      Predict: 0.0\n   Else (feature 2 > 49.5)\n    If (feature 1 <= 1.5)\n     Predict: 1.0\n    Else (feature 1 > 1.5)\n     If (feature 5 <= 24.075)\n      Predict: 1.0\n     Else (feature 5 > 24.075)\n      If (feature 2 <= 54.5)\n       Predict: 1.0\n      Else (feature 2 > 54.5)\n       Predict: 0.0\n  Else (feature 0 > 0.5)\n   If (feature 5 <= 27.825)\n    If (feature 5 <= 18.375)\n     Predict: 0.0\n    Else (feature 5 > 18.375)\n     If (feature 2 <= 2.5)\n      Predict: 1.0\n     Else (feature 2 > 2.5)\n      Predict: 0.0\n   Else (feature 5 > 27.825)\n    If (feature 3 <= 2.5)\n     If (feature 2 <= 17.5)\n      If (feature 1 <= 2.5)\n       Predict: 1.0\n      Else (feature 1 > 2.5)\n       Predict: 0.0\n     Else (feature 2 > 17.5)\n      If (feature 5 <= 30.5979)\n       Predict: 1.0\n      Else (feature 5 > 30.5979)\n       Predict: 0.0\n    Else (feature 3 > 2.5)\n     Predict: 0.0\n```"],"metadata":{}},{"cell_type":"markdown","source":["#### Comment on the tree top level split in this cell:\n\nVariable being split = feature 1 \nValue determining the left / right split: left if <= 0.5 and right if > 0.5\n\nAccording to the example tree:\n    The root is feature 1, first it would compare feature 1 values:\n    If feature 1 <= 0.5, it will execute if part, otherwise else for feature 1 > .5"],"metadata":{}},{"cell_type":"markdown","source":["# Question 6:  10 pts.\nCreate a spark GBTClassifier using all default parameters.  Train the model and calculate the AUC using a BinaryClassificationEvaluator."],"metadata":{}},{"cell_type":"code","source":["# Your Code Here\ngbt_assembler = VectorAssembler(inputCols=training_df.columns[0:6], outputCol=\"features\")\ngbt = GBTClassifier(labelCol=\"survived\", featuresCol=\"features\", seed = 1234)\ngbt_pipeline = Pipeline(stages=[gbt_assembler, gbt]).fit(training_df)\n\nbce = BinaryClassificationEvaluator(labelCol='survived')\ndefault_gbt_model = bce.evaluate(gbt_pipeline.transform(testing_df))\nprint(default_gbt_model)\n#raise NotImplementedError()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.8427267458707624\n</div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["# Question 7:  10 pts.\nUse spark GBTClassifier, ParamGridBuilder, and CrossValidator objects to perform a GBT grid search.  Use 3 fold cross validation and a BinaryClassificationEvaluator to evaluate the results.  The goal is to see if you can improve upon the AUC score produced by the default GBT model above.  Obvious choices for the grid search include maximum tree depth, maxIter, and stepSize.  Note that maxIter and stepSize are not very well explained in the documentation.  maxIter indicates the number of trees to grow in the series and stepSize is the multiplier for each tree.  If stepSize is reduced then maxIter probably needs to increase to create an equivalent number of trees. You are free to experiment with any search parameters you wish.  It is also okay to experiment with maxIter and stepSize outside of the grid if you wish as long as you comment what you are doing."],"metadata":{}},{"cell_type":"code","source":["# Your Code Here\nparamGrid = (ParamGridBuilder()\n             .addGrid(gbt.maxDepth, [2, 4, 6])\n             .addGrid(gbt.maxBins, [20, 30])\n             .addGrid(gbt.maxIter, [10, 15])\n             .addGrid(gbt.stepSize,[0.4, 0.6, 0.8])\n             .build())\n\ncv_gbt= CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=bce, numFolds=3)\n# Run cross validations\ncv_gbt_Model = Pipeline(stages = [gbt_assembler, cv_gbt]).fit(training_df)\n#raise NotImplementedError()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:791: UserWarning: Can not find mlflow. To enable mlflow logging, install MLflow library from PyPi.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["gbt_prediction = cv_gbt_Model.transform(testing_df)\ngbt_prediction.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------+----+-----+-----+--------+--------+--------------------+--------------------+--------------------+----------+\nis_male|pclass| age|sibsp|parch|    fare|survived|            features|       rawPrediction|         probability|prediction|\n+-------+------+----+-----+-----+--------+--------+--------------------+--------------------+--------------------+----------+\n    0.0|     1| 2.0|    1|    2|  151.55|       0|[0.0,1.0,2.0,1.0,...|[-2.2574289676959...|[0.01082666040651...|       1.0|\n    0.0|     1|16.0|    0|    0|    86.5|       1|[0.0,1.0,16.0,0.0...|[-1.3937543004315...|[0.05800293201032...|       1.0|\n    0.0|     1|16.0|    0|    1|    39.4|       1|[0.0,1.0,16.0,0.0...|[-1.3464377067196...|[0.06339507290045...|       1.0|\n    0.0|     1|16.0|    0|    1| 57.9792|       1|[0.0,1.0,16.0,0.0...|[-1.5263194037540...|[0.04510368525453...|       1.0|\n    0.0|     1|17.0|    1|    0|   108.9|       1|[0.0,1.0,17.0,1.0...|[-1.3421890995919...|[0.06390148035055...|       1.0|\n    0.0|     1|21.0|    2|    2| 262.375|       1|[0.0,1.0,21.0,2.0...|[-1.2899337966874...|[0.07044540076237...|       1.0|\n    0.0|     1|22.0|    0|    1| 61.9792|       1|[0.0,1.0,22.0,0.0...|[-1.5263194037540...|[0.04510368525453...|       1.0|\n    0.0|     1|22.0|    0|    2|    49.5|       1|[0.0,1.0,22.0,0.0...|[-1.3464377067196...|[0.06339507290045...|       1.0|\n    0.0|     1|23.0|    1|    0| 82.2667|       1|[0.0,1.0,23.0,1.0...|[-1.3421890995919...|[0.06390148035055...|       1.0|\n    0.0|     1|23.0|    1|    0| 113.275|       1|[0.0,1.0,23.0,1.0...|[-1.3421890995919...|[0.06390148035055...|       1.0|\n    0.0|     1|24.0|    0|    0|    69.3|       1|[0.0,1.0,24.0,0.0...|[-1.3937543004315...|[0.05800293201032...|       1.0|\n    0.0|     1|27.0|    1|    1|247.5208|       1|[0.0,1.0,27.0,1.0...|[-1.4747542029144...|[0.04975975081458...|       1.0|\n    0.0|     1|28.0|    3|    2|   263.0|       1|[0.0,1.0,28.0,3.0...|[-1.1605800636777...|[0.08938558484322...|       1.0|\n    0.0|     1|29.0|    0|    0|211.3375|       1|[0.0,1.0,29.0,0.0...|[-1.3117529837494...|[0.06764085230529...|       1.0|\n    0.0|     1|30.0|    0|    0|    93.5|       1|[0.0,1.0,30.0,0.0...|[-1.3117529837494...|[0.06764085230529...|       1.0|\n    0.0|     1|30.0|    0|    0| 106.425|       1|[0.0,1.0,30.0,0.0...|[-1.3117529837494...|[0.06764085230529...|       1.0|\n    0.0|     1|32.0|    0|    0| 76.2917|       1|[0.0,1.0,32.0,0.0...|[-1.3117529837494...|[0.06764085230529...|       1.0|\n    0.0|     1|33.0|    0|    0|    86.5|       1|[0.0,1.0,33.0,0.0...|[-1.3117529837494...|[0.06764085230529...|       1.0|\n    0.0|     1|35.0|    0|    0|135.6333|       1|[0.0,1.0,35.0,0.0...|[-1.3117529837494...|[0.06764085230529...|       1.0|\n    0.0|     1|35.0|    1|    0|    52.0|       1|[0.0,1.0,35.0,1.0...|[-1.2601877829098...|[0.07444206437842...|       1.0|\n+-------+------+----+-----+-----+--------+--------+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["# Question 8 10 pts\nPrint the AUC and hyper parameters of the best GBT model.  Add comments to the markdown cell which indicate the specific search parameter you used and how they relate to the GBT training process."],"metadata":{}},{"cell_type":"code","source":["print(bce.evaluate(cv_gbt_Model.transform(testing_df)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.8471216072635952\n</div>"]}}],"execution_count":34},{"cell_type":"code","source":["# Your Code Here\nbest_model_gbt = cv_gbt_Model.stages[-1].bestModel\nbest_model_gbt.extractParamMap()\n#raise NotImplementedError()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[122]: {Param(parent=&#39;GBTClassifier_36389df1deaa&#39;, name=&#39;cacheNodeIds&#39;, doc=&#39;If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.&#39;): False,\n Param(parent=&#39;GBTClassifier_36389df1deaa&#39;, name=&#39;checkpointInterval&#39;, doc=&#39;set checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext&#39;): 10,\n Param(parent=&#39;GBTClassifier_36389df1deaa&#39;, name=&#39;featureSubsetStrategy&#39;, doc=&#39;The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].&#39;): &#39;all&#39;,\n Param(parent=&#39;GBTClassifier_36389df1deaa&#39;, name=&#39;featuresCol&#39;, doc=&#39;features column name&#39;): &#39;features&#39;,\n Param(parent=&#39;GBTClassifier_36389df1deaa&#39;, name=&#39;labelCol&#39;, doc=&#39;label column name&#39;): &#39;survived&#39;,\n Param(parent=&#39;GBTClassifier_36389df1deaa&#39;, name=&#39;lossType&#39;, doc=&#39;Loss function which GBT tries to minimize (case-insensitive). Supported options: logistic&#39;): &#39;logistic&#39;,\n Param(parent=&#39;GBTClassifier_36389df1deaa&#39;, name=&#39;maxBins&#39;, doc=&#39;Max number of bins for discretizing continuous features.  Must be at least 2 and at least number of categories for any categorical feature.&#39;): 20,\n Param(parent=&#39;GBTClassifier_36389df1deaa&#39;, name=&#39;maxDepth&#39;, doc=&#39;Maximum depth of the tree. (Nonnegative) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.&#39;): 2,\n Param(parent=&#39;GBTClassifier_36389df1deaa&#39;, name=&#39;maxIter&#39;, doc=&#39;maximum number of iterations (&gt;= 0)&#39;): 10,\n Param(parent=&#39;GBTClassifier_36389df1deaa&#39;, name=&#39;maxMemoryInMB&#39;, doc=&#39;Maximum memory in MB allocated to histogram aggregation.&#39;): 256,\n Param(parent=&#39;GBTClassifier_36389df1deaa&#39;, name=&#39;minInfoGain&#39;, doc=&#39;Minimum information gain for a split to be considered at a tree node.&#39;): 0.0,\n Param(parent=&#39;GBTClassifier_36389df1deaa&#39;, name=&#39;minInstancesPerNode&#39;, doc=&#39;Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Must be at least 1.&#39;): 1,\n Param(parent=&#39;GBTClassifier_36389df1deaa&#39;, name=&#39;predictionCol&#39;, doc=&#39;prediction column name&#39;): &#39;prediction&#39;,\n Param(parent=&#39;GBTClassifier_36389df1deaa&#39;, name=&#39;seed&#39;, doc=&#39;random seed&#39;): 1234,\n Param(parent=&#39;GBTClassifier_36389df1deaa&#39;, name=&#39;stepSize&#39;, doc=&#39;Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.&#39;): 0.4,\n Param(parent=&#39;GBTClassifier_36389df1deaa&#39;, name=&#39;subsamplingRate&#39;, doc=&#39;Fraction of the training data used for learning each decision tree, in range (0, 1].&#39;): 1.0}</div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["Add search parameter comments here\n\n```\nmaxDepth(2): Increasing the depth makes the model more expressive and powerful. However, deep trees take longer to train and are also more prone to overfitting.\nsetStepSize(0.4): Step size means the learning rate for the algorithm. This rate should be appropriatelt determined so that it converges to the local minima without taking too much time.\nmaxBins(30): Increasing maxBins allows the algorithm to consider more split candidates and make fine-grained split decisions. \nmaxIter(15): Each iteration produces one tree. Increasing this number makes the model more expressive, improving training data accuracy.\n```"],"metadata":{}},{"cell_type":"markdown","source":["# Question 9: 10 pts\nCreate a pandas dataframe `feature_importance` with the columns `feature` and `importance` which contains the names of the features (`is_male`, `pclass`, etc.) and their feature importance as determined by the GBT algorithm. Sort the dataframe by `importance` in descending order.  Add comments about the feature importances similar to what you did for random forest above.  Also, compare the feature importances to random forest - how different or similar are the feature importances between GBT and random forest."],"metadata":{}},{"cell_type":"code","source":["# Your code here\n# Your code here\nimportances = gbt_pipeline.stages[-1].featureImportances.toArray()\nfeatures = training_df.columns[0:6]\n\nfeature_importance = pd.DataFrame(list(zip(features,importances)),\n                                 columns = ['feature', 'importance']).sort_values('importance',ascending = False)\nfeature_importance\n#raise NotImplementedError()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>age</td>\n      <td>0.358323</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>fare</td>\n      <td>0.320546</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>is_male</td>\n      <td>0.093107</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sibsp</td>\n      <td>0.089562</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>parch</td>\n      <td>0.070254</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pclass</td>\n      <td>0.068207</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":38},{"cell_type":"markdown","source":["### Add feature importance comments here:\n\nThe GBT model has given the most importance to 'age' which makes sense to me because the younger you are, the more prone your body is to take a shock well than older people. \n\nThe second important feature 'fare' makes sense, because it might so happen that the first class people who had higher fares were given first priority to save their life than economy fare people. Or they might even have more number of life boats reserved for them than the economy passengers. \n\nThe third most important feature is 'is_male' which shows that gender determines the probability of surviving.\n\nOur model also suggests that the number of siblings/spouses and Parents/Children had not much of a significance to the survival rate just like \n\nAnd lastly, our model suggests having the feature 'pclass' important, which makes sense based on the above discussed factors."],"metadata":{}},{"cell_type":"markdown","source":["# Question 10: 10 pts\nCalculate the AUC of the best random forest and GBT models using the held out test data.  Which model produces the best accuracy."],"metadata":{}},{"cell_type":"code","source":["# Your code here\nprint(\"Best Model on Testing DF with RF: \",bce.evaluate(cv_rf_Model.transform(testing_df)))\nprint(\"Best Model on Testing DF with GBT: \",bce.evaluate(cv_gbt_Model.transform(testing_df)))\n#raise NotImplementedError()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Best Model on Testing DF with RF:  0.8601130107215299\nBest Model on Testing DF with GBT:  0.8471216072635952\n</div>"]}}],"execution_count":41},{"cell_type":"markdown","source":["### Which model produced the best results?\nFor me, the random forest's best model gave me the best accuracy as evident from above."],"metadata":{}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.4","nbconvert_exporter":"python","file_extension":".py"},"name":"random_forest","notebookId":3534793031034439,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"celltoolbar":"Edit Metadata"},"nbformat":4,"nbformat_minor":0}
